{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24234,"status":"ok","timestamp":1651874558688,"user":{"displayName":"Assaf La","userId":"07226046456222457646"},"user_tz":-180},"id":"OMIVlc1Bg_4M","outputId":"97907922-7c31-4b47-cda6-60b7a05d4c2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1216,"status":"ok","timestamp":1651874559897,"user":{"displayName":"Assaf La","userId":"07226046456222457646"},"user_tz":-180},"id":"ZMRxQXMsRamB","outputId":"e0a09a49-6dc0-42a9-aeaf-c1e53d862d1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/afeka/Project/code/PTEEnet/PTEEnet\n"]}],"source":["#@title cd\n","%pwd\n","%cd /content/drive/MyDrive/afeka/Project/code/PTEEnet/PTEEnet"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11066,"status":"ok","timestamp":1651874574810,"user":{"displayName":"Assaf La","userId":"07226046456222457646"},"user_tz":-180},"id":"UVXtOWnTB_MB","outputId":"424b321b-f6fe-4bc4-eefa-b590163b9965"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting chainer\n","  Downloading chainer-7.8.1.tar.gz (1.0 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 122 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 133 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 163 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 184 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 194 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 225 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 235 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 245 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 296 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 307 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 368 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 409 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 419 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 430 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 471 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 481 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 491 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 532 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 542 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 593 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 604 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 614 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 645 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 655 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 665 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 675 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 686 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 706 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 727 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 737 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 768 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 778 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 788 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 798 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 808 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 819 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 829 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 839 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 849 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 860 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 870 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 890 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 901 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 921 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 931 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 942 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 952 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 962 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 972 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 983 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 993 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0 MB 9.9 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from chainer) (57.4.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from chainer) (4.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from chainer) (3.6.0)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from chainer) (1.21.6)\n","Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from chainer) (3.17.3)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from chainer) (1.15.0)\n","Building wheels for collected packages: chainer\n","  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chainer: filename=chainer-7.8.1-py3-none-any.whl size=967740 sha256=bdc7889fa3a6c2411599a011ecffad97380c91a5f0be77dac8ffb3f6e37eae77\n","  Stored in directory: /root/.cache/pip/wheels/c8/6a/6f/fd563166cc597e5206e375ea074ea836e5db5dd58421215672\n","Successfully built chainer\n","Installing collected packages: chainer\n","Successfully installed chainer-7.8.1\n","\u001b[K     |████████████████████████████████| 793 kB 9.4 MB/s \n","\u001b[K     |████████████████████████████████| 381 kB 78.5 MB/s \n","\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.33.0 which is incompatible.\u001b[0m\n"]}],"source":["#@title install packages\n","%pip install chainer\n","%pip install -Uqq ipdb"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2575,"status":"ok","timestamp":1651874577381,"user":{"displayName":"Assaf La","userId":"07226046456222457646"},"user_tz":-180},"id":"fOMPfWSZhFlH","outputId":"b803f5ca-0244-4e15-828e-cde2d1feb90e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/chainer/_environment_check.py:75: UserWarning: \n","--------------------------------------------------------------------------------\n","CuPy (cupy-cuda111) version 9.4.0 may not be compatible with this version of Chainer.\n","Please consider installing the supported version by running:\n","  $ pip install 'cupy-cuda111>=7.7.0,<8.0.0'\n","\n","See the following page for more details:\n","  https://docs.cupy.dev/en/latest/install.html\n","--------------------------------------------------------------------------------\n","\n","  requirement=requirement, help=help))\n"]},{"output_type":"stream","name":"stdout","text":["GPU availability: True\n","cuDNN availablility: True\n","Fri May  6 22:02:57 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Your runtime has 27.3 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}],"source":["#@title read gpu details\n","import chainer\n","#chainer.print_runtime_info()\n","\n","print('GPU availability:', chainer.cuda.available)\n","print('cuDNN availablility:', chainer.cuda.cudnn_enabled)\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Km0SuBYdph8B"},"outputs":[],"source":["#import os\n","#assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n","#!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzVYRVokq8G2"},"outputs":[],"source":["#import torch_xla\n","#import torch_xla.core.xla_model as xm"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"YxxBGSa5UqrP","executionInfo":{"status":"ok","timestamp":1651874586099,"user_tz":-180,"elapsed":8722,"user":{"displayName":"Assaf La","userId":"07226046456222457646"}}},"outputs":[],"source":["#@title imports\n","\"\"\"\n","high level support for doing this and that.\n","\"\"\"\n","from __future__ import print_function\n","import time\n","import csv\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from scipy import stats\n","from init import initializer\n","from eenet import EENet\n","from custom_eenet import CustomEENet\n","from gatednet import GatedNet\n","import matplotlib.pylab as plt\n","import loss_functions\n","import utils\n","import config\n","import gc\n","import sys\n","import math\n","import os\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision.transforms.functional as tf\n","import matplotlib.pyplot as plt\n","import ipdb\n","\n","import xgboost as xgb\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import accuracy_score\n","import joblib as jobl\n","from collections import Counter\n","from matplotlib import pyplot\n","from numpy import where\n","from imblearn.over_sampling import RandomOverSampler \n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.over_sampling import SMOTE\n","from sklearn.model_selection import train_test_split\n","from xgboost.sklearn import XGBRegressor\n","\n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RV2LWymWDv__"},"outputs":[],"source":["#@title add noise\n","def add_noise(args, image):\n","    mean_i = (0.4914, 0.4822, 0.4465)\n","    std_i = (0.2023, 0.1994, 0.2010)\n","    #ipdb.set_trace(context=6)\n","\n","    for t, m, s in zip(image, mean_i, std_i):\n","        t.mul_(s).add_(m)\n","    \n","    \"\"\"\n","%matplotlib inline\n","\n","    # Generate the noise as you did\n","    im = image[0].detach().cpu().numpy()\n","    #ipdb.set_trace(context=6)\n","    im = np.transpose(im,(1,2,0))\n","    plt.imshow(im) \n","    plt.show()\n","    #ipdb.set_trace(context=6)\n","    \"\"\"\n","\n","    _, ch, row, col = image.shape\n","    mean_n = 0\n","    var_n = 0.1\n","    sigma = var_n**0.5\n","    noise = torch.normal(mean_n,sigma,(1, ch, row, col))\n","    snr = 10.0 ** (args.noise_snr / 10.0)\n","    #ipdb.set_trace(context=6)\n","    # work out the current SNR\n","    current_snr = torch.mean(image) / torch.std(noise)\n","\n","    # scale the noise by the snr ratios (smaller noise <=> larger snr)\n","    noise = noise * (current_snr / snr)\n","    img_noise = image + noise\n","    \n","    \"\"\"\n","    im = img_noise[0].detach().cpu().numpy()\n","    im = np.transpose(im,(1,2,0))\n","    plt.imshow(im) \n","    plt.show()\n","    ipdb.set_trace(context=6)\n","    \"\"\"\n","\n","    #utils.save_image(args, img_noise, 'image1')\n","    img_noise = tf.normalize(img_noise, mean_i, std_i)\n","\n","    # return the new signal with noise\n","    return img_noise"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WVgJ_onrXSUZ"},"outputs":[],"source":["#@title plot_ee_histogram\n","def plot_ee_histogram(args, val_confs, type_loader, dir):\n","    %matplotlib inline\n","    x_pos = np.arange(args.num_ee+1)\n","    #print(x_pos)\n","    print(val_confs)\n","    bar = plt.bar(x_pos, val_confs, align='center')\n","    plt.xlabel('Exit number')\n","    plt.ylabel('# of Samples in Exit')\n","    plt.title(args.dataset + ' samples distribution in exits using ' + args.model + \n","              ' model\\n' + 'SNR=' + str(args.noise_snr) + 'dB')\n","    plt.ylim(0, max(val_confs))\n","    plt.savefig(dir + '/ee_hist_' + type_loader + '.' + str(args.loss_threshold) + '.png')\n","    plt.show()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"GHyuN338603H"},"outputs":[],"source":["#@title plot_noise_exits\n","import plotly.graph_objects as go\n","import matplotlib\n","\n","#@title plot_noise_exits\n","def plot_noise_exits_box(args, type_loader):\n","    %matplotlib inline\n","    x_pos = np.arange(args.num_noise_levels)\n","\n","    for thresh in range(1, 13, 1):\n","        loss_threshold = thresh/10\n","        data_file = args.relative_losses_dir + '/noise_exits_' + type_loader + '.' + str(loss_threshold) + '.pt'\n","        noise_exits = torch.load(data_file)\n","        #noise_exits = np.array(noise_exits)\n","\n","        #ipdb.set_trace(context=6)\n","        mean=np.mean(noise_exits,axis=1)\n","        print(mean)\n","\n","        fig, ax = plt.subplots()#figure(figsize =(10, 7))\n","    \n","        # Creating axes instance\n","        ax.set_title(args.dataset + ' samples distribution in exits using ' + args.model + ' with threshold=' + str(loss_threshold))\n","        ax.set_xlabel('noise index')\n","        ax.set_ylabel('# exits')\n","        \n","        # Creating plot\n","        bp = ax.boxplot(noise_exits)\n","        \n","        # show plot\n","        plt.show()\n","        plt.savefig(args.relative_losses_dir + '/bp_noises_exits_' + '.' + str(loss_threshold) + '.png')\n","        \n","def plot_noise_exits_scatter(args, type_loader):\n","    %matplotlib inline\n","    #x_pos = np.arange(args.num_noise_levels)\n","    noise_levels = np.arange(args.min_noise_snr, args.max_noise_snr,\n","        (args.max_noise_snr - args.min_noise_snr)/args.num_noise_levels) \n","\n","    x = noise_levels\n","    selected_losses = np.array([0.3, 0.5, 0.7, 0.9])\n","    colors = ['green', 'black', 'red', 'blue']\n","    fig = go.Figure()\n","\n","    #for thresh in selected_losses:\n","    for idx, thresh in enumerate(selected_losses):\n","        loss_threshold = thresh#/10\n","        data_file = args.relative_losses_dir + '/noise_exits_' + type_loader + '.' + str(loss_threshold) + '.pt'\n","        data = torch.load(data_file)\n","        #noises_data.append(data)\n","\n","        #mean=np.median(data,axis=1)\n","        mean=np.mean(data,axis=1)\n","        std=np.std(data,axis=1)\n","        q1=np.quantile(data,q=0.25,axis=1)\n","        q3=np.quantile(data,q=0.75,axis=1)\n","        \"\"\"\n","        fig = go.Figure()\n","        \n","        fig.add_trace(go.Scatter(x=x, y=q3,\n","                                mode='lines',\n","                                line=dict(color='red',width =0.1),\n","                                name='upper bound'))\n","        \"\"\"\n","        fig.add_trace(go.Scatter(x=x, y=mean,\n","                                mode='lines',\n","                                line=dict(color=colors[idx]),\n","                                #fill='tonexty',\n","                                name='threshold ' + str(loss_threshold)))\n","        \"\"\"\n","        fig.add_trace(go.Scatter(x=x, y=q1,\n","                                mode='lines',\n","                                line=dict(color='blue', width =0.1),\n","                                fill='tonexty',\n","                                name='lower bound'))\n","        \"\"\"\n","        #ipdb.set_trace(context=6)\n","    fig.update_layout(\n","        title=args.dataset + ' samples distribution in exits using ' + args.model,# + ' with threshold=' + str(loss_threshold),\n","        xaxis_title=\"noise level in SNR (dB)\",\n","        yaxis_title=\"# exit\",\n","        xaxis = dict(\n","            tickmode = 'array',\n","            tickvals = noise_levels,\n","        ),\n","        width=700,\n","        height=500)\n","\n","    #plt.show()\n","    fig.show()\n","    plt.savefig(args.relative_losses_dir + '/bp_noises_exits_' + '.' + str(args.loss_threshold) + '.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WCarzY5EGFlc"},"outputs":[],"source":["#@title exit_flow_analysis\n","def exit_flow_analysis(args, model, val_loader):\n","    \"\"\"examine the model output.\n","    Arguments are\n","    * args:         command line arguments entered by user.\n","    * model:        convolutional neural network model.\n","    * train_loader: train data loader.\n","    This examines the outputs of already trained model.\n","    \"\"\"\n","    model.eval()\n","    val_confs_true = []\n","    val_confs_false = []\n","    vals_len_true = np.zeros(args.num_ee+1)\n","    vals_len_false = np.zeros(args.num_ee+1)\n","    for i in range(args.num_ee+1):\n","        val_confs_true.append([])\n","        val_confs_false.append([])\n","    \n","    #val_confs[0].append('aa2')\n","    #val_confs = np.empty((args.num_ee, 0), float)#np.zeros(len(val_loader.dataset))\n","    #i = 0\n","    \"\"\"\n","    print(args.results_dir+'/pred_vs_conf.csv')\n","    experiment = open(args.results_dir+'/pred_vs_conf.csv', 'w', newline='')\n","    recorder = csv.writer(experiment, delimiter=',')\n","    recorder.writerow(['target',\n","                       'start_pred_seq',\n","                       'start_conf_seq',\n","                       'start_exit_seq',\n","                       'actual_pred',\n","                       'actual_conf',\n","                       'actual_exit'])\n","    \"\"\"\n","    \n","    print('add noise w/ SNR=' + str(args.testing_snr) + 'dB')\n","    \n","    with torch.no_grad():\n","        for data, target in val_loader:\n","            data = add_noise(args, data)\n","            \n","            data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n","            pred, conf, cost, idx = model(data)\n","            loss = F.nll_loss(pred.log(), target)\n","\n","            pred = pred.max(1, keepdim=True)[1].item()\n","            conf = conf.item()\n","            #print(loss)\n","            #conf = torch.max(confs).item()\n","            #i += 1\n","            #confs = [c.item() for c in confs]\n","            loss = loss.item()\n","            target = target.item()\n","            if target == pred:\n","                val_confs_true[idx].append(conf)\n","            else:\n","                val_confs_false[idx].append(conf)\n","\n","            #val_confs[idx].append(conf)\n","\n","    total_correct = 0\n","    for i in range(args.num_ee+1):\n","        total_correct += len(val_confs_true[i])\n","        if len(val_confs_true[i]) > 1:\n","            filename = 'test_true_confs_hist_ee' + str(i)\n","            vals_len_true[i] = len(val_confs_true[i])\n","            utils.plot_histogram(args, val_confs_true[i], 'conf', 100, filename, saveplot=True)\n","        if len(val_confs_false[i]) > 1:\n","            filename = 'test_false_confs_hist_ee' + str(i)\n","            vals_len_false[i] = len(val_confs_false[i])\n","            utils.plot_histogram(args, val_confs_false[i], 'conf', 100, filename, saveplot=True)\n","    flat_list_true = [item for sublist in val_confs_true for item in sublist]\n","    flat_list_false = [item for sublist in val_confs_false for item in sublist]\n","    \n","    utils.plot_histogram(args, flat_list_true, 'conf true', 100, 'global_conf_true', saveplot=True)\n","    utils.plot_histogram(args, flat_list_false, 'conf false', 100, 'global_conf_false', saveplot=True)\n","    print('total currect: ', total_correct)\n","    print('total acc : ', total_correct * 100 /10000)\n","    plot_ee_histogram(args, vals_len_true)\n","    plot_ee_histogram(args, vals_len_false)\n","    return"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"M0FWfwVdGNma","executionInfo":{"status":"ok","timestamp":1651874587050,"user_tz":-180,"elapsed":2,"user":{"displayName":"Assaf La","userId":"07226046456222457646"}}},"outputs":[],"source":["#@title Train\n","def train(args, model, train_loader, optimizer, exit_tags):\n","    \"\"\"train the model.\n","\n","    Arguments are\n","    * args:         command line arguments entered by user.\n","    * model:        convolutional neural network model.\n","    * train_loader: train data loader.\n","    * optimizer:    optimize the model during training.\n","    * epoch:        epoch number.\n","\n","    This trains the model and prints the results of each epochs.\n","    \"\"\"\n","    losses = []\n","    pred_losses = []\n","    cost_losses = []\n","    model.train()\n","    \n","    # actual training starts\n","    #for batch_id, ((data, noise), target) in enumerate(train_loader):\n","    for batch_id, (data, target, _) in enumerate(train_loader):\n","        # add noise to input data\n","        #ipdb.set_trace(context=6)\n","        #if args.add_noise != 0:\n","        #    data = add_noise(args, data)\n","        # fetch the current batch data\n","        data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n","        exit_tag = None\n","        if args.loss_func == 'v4':\n","            exit_tag = exit_tags[batch_id].to(args.device)\n","        optimizer.zero_grad()\n","        cum_loss = 0\n","\n","        # training settings for EENet based models\n","        if isinstance(model, (CustomEENet, EENet)):\n","            pred, conf, cost, _ = model(data)\n","            cost.append(1)#torch.tensor(1.0).to(args.device))\n","            \n","            if args.use_main_targets:\n","                _, target = torch.max(pred[args.num_ee], 1)\n","                #cum_loss, pred_loss, cost_loss = loss_functions.loss(args, exit_tag, pred, target, conf, cost)\n","                         \n","            if config.mode == 'train_main':\n","                cum_loss = F.nll_loss(pred[args.num_ee].log(), target)\n","                #cum_loss = F.cross_entropy(pred[args.num_ee], target)\n","                pred_loss = cum_loss\n","                cost_loss = 1.0\n","            elif config.mode == 'train_ee':\n","                #ipdb.set_trace(context=6)                \n","                cum_loss, pred_loss, cost_loss = loss_functions.loss(args, exit_tag, pred, target, conf, cost)\n","            else:\n","                print('Error: undefined mode..')\n","                return        \n","                \"\"\"\n","                if args.ee_disable:\n","                    cum_loss = F.nll_loss(pred[args.num_ee].log(), target)\n","                    #cum_loss = F.cross_entropy(pred[args.num_ee], target)\n","                    pred_loss = cum_loss\n","                    cost_loss = 1.0\n","                else:\n","                    cum_loss, pred_loss, cost_loss = loss_functions.loss(args, exit_tag, pred, target, conf, cost)\n","                    #cum_loss, loss = calc_loss(args, pred, conf, cost, target, exit_id)\n","                \"\"\"\n","\n","        # training settings for other models\n","        else:\n","            pred = model(data)\n","            cum_loss = F.cross_entropy(pred, target)\n","\n","        losses.append(float(cum_loss))\n","        pred_losses.append(float(pred_loss))\n","        cost_losses.append(float(cost_loss))\n","        cum_loss.backward()\n","        optimizer.step()\n","        \"\"\"\n","        for obj in gc.get_objects():\n","            try:\n","                if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n","                    print(type(obj), obj.size())\n","            except:\n","                pass\n","        \"\"\"\n","        #print(model.exits[1].classifier[0].weight)\n","\n","        # update the exit tags of inputs\n","    #print(pred_loss)\n","    #print('target')\n","    #print(target)\n","    if args.loss_func == 'v4' and args.ee_disable == False:\n","        for batch_id, (data, target) in enumerate(train_loader):\n","            data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n","            batch_size = len(exit_tags[batch_id])\n","            pred, _, cost = model(data)\n","\n","            if args.use_main_targets:\n","                _, target = torch.max(pred[args.num_ee], 1)\n","            exit_tags[batch_id] = loss_functions.update_exit_tags(args, batch_size,\n","                                                                  pred, target, cost)\n","\n","\n","    # print the training results of epoch\n","    result = {'train_loss': round(np.mean(losses), 4),\n","              'train_loss_sem': round(stats.sem(losses), 2),\n","              'pred_loss': round(np.mean(pred_losses), 4),\n","              'pred_loss_sem': round(stats.sem(pred_losses), 2),\n","              'cost_loss': round(np.mean(cost_losses), 4),\n","              'cost_loss_sem': round(stats.sem(cost_losses), 2)}\n","\n","    print('Train avg loss: {:.4f}'.format(result['train_loss']))\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ybzxK0yeUAOw"},"outputs":[],"source":["#@title train_gated_xgboost\n","\"\"\"\n","params={\n","'booster':'gbtree',\n","'objective': 'multi:softmax',\n","'num_class':11,\n","'gamma':0.6,#0.1,\n","'max_depth':10, #6\n","'lambda': 3,#2,\n","'alpha': 4, #new\n","'subsample':0.7,\n","'colsample_bytree':0.7,\n","'min_child_weight':1,\n","'silent':0 ,\n","'eta': 0.01, #0.01\n","'seed':1000,\n","'tree_method': 'gpu_hist',\n","'gpu_id': 0,\n","}\n","\"\"\"\n","depth = 8\n","params={\n","'booster':'gbtree',\n","'objective': 'multi:softmax',\n","'num_class':11,\n","'gamma':0.6,#0.1,\n","'max_depth':depth, #6\n","'lambda': 3,#2,\n","'alpha': 4, #new\n","'subsample':0.7,\n","'colsample_bytree':0.7,\n","'min_child_weight':1,\n","'silent':0 ,\n","'eta': 0.03, #0.01\n","'seed':1000,\n","'tree_method': 'gpu_hist',\n","'gpu_id': 0,\n","}\n","\n","#82%\n","\n","def evaluate(model, X, Y):#, weights):\n","    predicted_Y = model.predict(X)\n","    #ipdb.set_trace(context=6)\n","    print(predicted_Y)\n","    accuracy = accuracy_score(Y, predicted_Y)#, weights)\n","    return accuracy\n","\"\"\"\n","def produce_weights(data, size, num_of_classes):\n","    counter = Counter(data)\n","    weights = np.zeros(num_of_classes)\n","    weights_vec = np.zeros(size)\n","    for key in counter:\n","        #print(key)\n","        weights[int(key)] = counter[key]\n","    weights = 1 - weights / np.max(weights) + 0.001\n","    #ipdb.set_trace(context=6)\n","    #print(weights)\n","    for i, value in enumerate(data):\n","        weights_vec[i] = weights[value]\n","    return weights_vec\n","\"\"\" \n","\n","def train_gated_xgboost(args, noise_levels):\n","    log_file = open(args.gated_models_dir+'/train_gated_xgboost_log.txt', 'a', newline='')\n","    log_file.write(\"Training gated xgboost mode:\\n\")\n","    log_file.write(\"loss threshold: \" + str(args.loss_threshold) + \", SNR=\" + str(args.noise_snr) + \"\\n\")\n","    \n","    %matplotlib inline\n","\n","    type_loader = 'test'\n","    ground_truth_noisy_dir = args.ground_truths_dir + '/' + str(args.loss_threshold)# + '/' + 'snr_' + str(noise_levels[0])\n","    \"\"\"\n","    data_file_train = ground_truth_noisy_dir + '/datas_train.' + str(args.loss_threshold) + '.pt'\n","    data_train = torch.load(data_file_train)\n","    print('loaded data train at: ', data_file_train)\n","    \n","    gt_file_train = ground_truth_noisy_dir + '/exit_idx_predictions_train.' + str(args.loss_threshold) + '.pt'\n","    exits_ground_truth_train = torch.load(gt_file_train)\n","    print('loaded exits ground truths train at: ', gt_file_train)\n","    \"\"\"\n","    data_file_test = ground_truth_noisy_dir + '/datas_' + type_loader + '.' + str(args.loss_threshold) + '.pt'\n","    data_test = torch.load(data_file_test)\n","    print('loaded data test at: ', data_file_test)\n","    \n","    gt_file_test = ground_truth_noisy_dir + '/exit_idx_predictions_' + type_loader + '.' + str(args.loss_threshold) + '.pt'\n","    exits_ground_truth_test = torch.load(gt_file_test)\n","    print('loaded exits ground truths test at: ', gt_file_test)\n","    \n","    if 0:\n","        for m in range(args.num_noise_levels-1):\n","            ground_truth_noisy_dir = args.ground_truths_dir + '/' + str(args.loss_threshold) + '/' + 'snr_' + str(noise_levels[m+1])\n","            \"\"\"\n","            data_file_train = ground_truth_noisy_dir + '/datas_train.' + str(args.loss_threshold) + '.pt'\n","            data = torch.load(data_file_train)\n","            print('loaded data train at: ', data_file_train)\n","            data_train = np.concatenate((data_train, data), axis=0)\n","\n","            gt_file_train = ground_truth_noisy_dir + '/exit_idx_predictions_train.' + str(args.loss_threshold) + '.pt'\n","            data = torch.load(gt_file_train)\n","            print('loaded exits ground truths train at: ', gt_file_train)\n","            exits_ground_truth_train = np.concatenate((exits_ground_truth_train, data), axis=0)\n","            \"\"\"\n","            data_file_test = ground_truth_noisy_dir + '/datas_' + type_loader + '.' + str(args.loss_threshold) + '.pt'\n","            data = torch.load(data_file_test)\n","            print('loaded data test at: ', data_file_test)\n","            data_test = np.concatenate((data_test, data), axis=0)\n","\n","            gt_file_test = ground_truth_noisy_dir + '/exit_idx_predictions_' + type_loader + '.' + str(args.loss_threshold) + '.pt'\n","            data = torch.load(gt_file_test)\n","            print('loaded exits ground truths test at: ', gt_file_test)\n","            exits_ground_truth_test = np.concatenate((exits_ground_truth_test, data), axis=0)\n","        \n","    \"\"\"\n","    ipdb.set_trace(context=6)\n","    gt_file_train = args.ground_truths_dir + '/exit_idx_predictions_train.' + str(args.loss_threshold) + '.pt'\n","    exits_ground_truth_train = torch.load(gt_file_train)\n","    print('loaded exits ground truths at: ', gt_file_train)\n","    train_len = len(exits_ground_truth_train)\n","    #print('exits_ground_truth_train {:8d}'.format(train_len))\n","    \n","    gt_file_test = args.ground_truths_dir + '/exit_idx_predictions_test.' + str(args.loss_threshold) + '.pt'\n","    exits_ground_truth_test = torch.load(gt_file_test)\n","    print('loaded exits ground truths at: ', gt_file_test)\n","    test_len = len(exits_ground_truth_test)\n","    #print('exits_ground_truth_test {:8d}'.format(test_len))\n","    \n","    data_file_train = args.ground_truths_dir + '/datas_train.' + str(args.loss_threshold) + '.pt'\n","    data_train = torch.load(data_file_train)\n","    print('loaded data train at: ', data_file_train)\n","    data_train_len = len(data_train)\n","    #print('data_train_len {:8d}'.format(data_train_len))\n","    \n","    data_file_test = args.ground_truths_dir + '/datas_test.' + str(args.loss_threshold) + '.pt'\n","    data_test = torch.load(data_file_test)\n","    print('loaded data test at: ', data_file_test)\n","    data_test_len = len(data_test)\n","    #print('data_test_len {:8d}'.format(data_test_len))\n","    \"\"\"\n","    #ipdb.set_trace(context=6)\n","    #data_train = torch.stack(data_train, dim=0)\n","    #data_test = torch.stack(data_test, dim=0)\n","    #d1, d2, d3, d4 = data_train.shape\n","    #data_train = data_train.reshape((d1, d2*d3*d4))\n","    d1, d2, d3, d4 = data_test.shape\n","    data_test = data_test.reshape((d1, d2*d3*d4))\n","    #data_train = torch.flatten(torch.from_numpy(data_train), start_dim=1)\n","    #data_test = torch.flatten(torch.from_numpy(data_test), start_dim=1)\n","\n","    #data_train = data_train.cpu().detach().numpy()\n","    #data_test = data_test.cpu().detach().numpy()\n","    #exits_ground_truth_train = np.array(exits_ground_truth_train)\n","    exits_ground_truth_test = np.array(exits_ground_truth_test)\n","    #ipdb.set_trace(context=6)\n","    ros_over = RandomOverSampler(random_state=42)\n","    ros_under = RandomUnderSampler(random_state=42)\n","    sm = SMOTE(random_state=42)\n","    \n","    #data_train, exits_ground_truth_train = ros_over.fit_resample(data_train, exits_ground_truth_train)\n","    #data_test, exits_ground_truth_test = ros_over.fit_resample(data_test, exits_ground_truth_test)\n","    #data_train, exits_ground_truth_train = sm.fit_resample(data_train, exits_ground_truth_train)\n","    print('Resampled test dataset shape before %s' % Counter(exits_ground_truth_test))\n","\n","    data_test, exits_ground_truth_test = sm.fit_resample(data_test, exits_ground_truth_test)\n","    #print('Resampled train dataset shape %s' % Counter(exits_ground_truth_train))\n","    print('Resampled test dataset shape %s' % Counter(exits_ground_truth_test))\n","\n","    X_train, X_test, y_train, y_test = train_test_split(data_test, exits_ground_truth_test, test_size=0.33, random_state=42)\n","    data_test = None\n","    exits_ground_truth_test = None\n","    \n","    #ipdb.set_trace(context=6)\n","    \n","    #xgb_train = xgb.DMatrix(X_train, label=y_train)#, weight=weights_vec_train)\n","    #xgb_val = xgb.DMatrix(X_test, label=y_test)#, weight=weights_vec_test)\n","    #watchlist = [(xgb_train, 'train'), (xgb_val, 'val')]\n","    \n","    num_rounds = 1500\n","    #depth = 4\n","    # Training phase\n","    print(\"building model...\")\n","    XGB = XGBRegressor(tree_method = \"gpu_hist\", objective = \"multi:softmax\", num_class=11,\\\n","                       single_precision_histogram=True, max_depth=args.max_depth, eval_metric=[\"merror\"],\\\n","                       n_estimators=num_rounds, verbosity=1)\n","    #print(XGB)\n","    eva_set = [(X_test, y_test)]\n","    X_test = None\n","    y_test = None\n","    #ipdb.set_trace(context=6)\n","    \n","    #XGB = XGB.fit(X_train, y_train, eval_metric=\"auc\", early_stopping_rounds=100, verbose=True)\n","    XGB = XGB.fit(X_train, y_train, verbose=True, early_stopping_rounds=5, eval_set=eva_set)\n","    #score = XGB.score(X_train, y_train)  \n","    #print(\"Training score: \", score)\n","    #plst = list(params.items())\n","    #gbm = xgb.train(plst, xgb_train, num_rounds, watchlist,early_stopping_rounds=100)\n","    print(\"saving model...\")\n","    log_file.write(\"saving model...\\n\")\n","    XGB.save_model(args.gated_models_dir + '/xg_model.' + str(args.loss_threshold) + '.depth' + str(args.max_depth) + '.json')\n","    #jobl.dump(gbm, args.gated_models_dir + '/xg_model.' + str(args.loss_threshold) + '.bin')\n","    #validation_accuracy = evaluate(gbm, xgb_val, y_test)#, weights_vec_test)\n","    #validation_accuracy = evaluate(XGB, X_test, y_test)#, weights_vec_test)\n","    \n","    #print(\"  - validation accuracy = {0:.4f}\".format(validation_accuracy))\n","    #log_file.write(\"validation accuracy: \" + str(validation_accuracy) + \"\\n\")\n","    log_file.close()\n","    return\n","    #return validation_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CeZ2Q0S4foIz"},"outputs":[],"source":["#@title train_gating_model\n","from gatednet import gnet, gnet_s, gnet_l\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","\n","def chunks(lst, n):\n","    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n","    for i in range(0, len(lst), n):\n","        yield lst[i:i + n]\n","        \n","def train_gating_model(args, main_model, train_loader, test_loader, trainset):    \n","    \n","    gt_file_train = 'gated_ground_truths/exit_idx_predictions_train.' + str(args.loss_threshold) + '.pt'\n","    exits_ground_truth_train = torch.load(gt_file_train)\n","    print('loaded exits ground truths at: ', gt_file_train)\n","    train_len = len(exits_ground_truth_train)\n","    print('exits_ground_truth_train {:8d}'.format(train_len))\n","    \n","    gt_file_test = 'gated_ground_truths/exit_idx_predictions_test.' + str(args.loss_threshold) + '.pt'\n","    exits_ground_truth_test = torch.load(gt_file_test)\n","    print('loaded exits ground truths at: ', gt_file_test)\n","    test_len = len(exits_ground_truth_test)\n","    print('exits_ground_truth_test {:8d}'.format(test_len))\n","    \n","    data_file_train = 'gated_ground_truths/datas_train.' + str(args.loss_threshold) + '.pt'\n","    data_train = torch.load(data_file_train)\n","    print('loaded data train at: ', data_file_train)\n","    data_train_len = len(data_train)\n","    print('data_train_len {:8d}'.format(data_train_len))\n","    \n","    data_file_test = 'gated_ground_truths/datas_test.' + str(args.loss_threshold) + '.pt'\n","    data_test = torch.load(data_file_test)\n","    print('loaded data test at: ', data_file_test)\n","    data_test_len = len(data_test)\n","    print('data_test_len {:8d}'.format(data_test_len))\n","    \n","    model = gnet((3, 32, 32), args.num_ee + 1, args.filters)\n","    #model = gnet_s((3, 32, 32), args.num_ee + 1, args.filters)\n","    #model = gnet_l((3, 32, 32), args.num_ee + 1)\n","    main_model.eval()\n","    main_model.set_full_flow(True)\n","\n","    #args.device = 'cpu'\n","    model = model.to(args.device)\n","    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n","    #optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    exits_ground_truth_train = torch.LongTensor(exits_ground_truth_train)\n","    #exit_gt_batches_train = list(chunks(exits_ground_truth_train, args.batch_size))\n","    exits_ground_truth_test = torch.LongTensor(exits_ground_truth_test)\n","    #exit_gt_batches_test = list(chunks(exits_ground_truth_test, args.batch_size))\n","    \n","    #print(data_train[0].shape)\n","    #data_train = torch.Tensor(data_train)\n","    data_train = torch.stack(data_train, dim=0)\n","    data_test = torch.stack(data_test, dim=0)\n","    print(data_train.shape)\n","    \n","    dataset_train = TensorDataset(data_train, exits_ground_truth_train)\n","    loader_train_new = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True)\n","    dataset_test = TensorDataset(data_test, exits_ground_truth_test)\n","    loader_test_new = DataLoader(dataset_test, batch_size=args.batch_size, shuffle=True)\n","            \n","    print('Running for {:5d} epochs'.format(args.epochs))\n","    try:\n","        if not args.no_tensorboard:\n","            writer = SummaryWriter('../runs/eenet_experiment_1')\n","        torch.cuda.empty_cache()\n","        for epoch in range(args.start_epoch, args.epochs + 1):\n","            print('{:3d}:'.format(epoch), end='')\n","            losses = []\n","            #i = 0\n","            model.train()\n","            for batch_id, (data, target) in enumerate(loader_train_new):\n","                data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n","                #if batch_id * args.batch_size > train_len:\n","                #    break\n","                optimizer.zero_grad()\n","                pred = model(data)\n","                #ipdb.set_trace(context=6)   \n","                #loss = criterion(pred, target)\n","                loss = F.nll_loss(pred.log(), target)\n","                loss.backward()\n","                optimizer.step()\n","\n","                losses.append(float(loss))\n","                #else:\n","                #    print('here')\n","                \n","                  \n","            print('Train avg loss: {:.4f}'.format(round(np.mean(losses), 4)))\n","            model.eval()\n","            #print('here')\n","            accs = []\n","            correct = 0\n","            batch_id = 0\n","            with torch.no_grad():\n","                #print('here1')\n","            \n","                for batch_id, (data, target) in enumerate(loader_test_new):\n","                    data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n","                    #if batch_id * args.batch_size > test_len:\n","                    #    break\n","                    #labels = exit_gt_batches_test[batch_id].to(args.device)\n","                    \n","                    pred = model(data)\n","                    main_preds, _, _, _ = main_model(data)\n","                    #ipdb.set_trace(context=6) \n","                    _, predicted = torch.max(pred.data, 1)\n","                    #_, main_predicted = torch.max(main_preds.data, 1)\n","                    \n","                    #correct += (predicted == target).sum().item()\n","                    pred = pred.max(1, keepdim=True)[1]\n","                    acc = pred.eq(target.view_as(pred)).sum().item()\n","                    accs.append(acc*100.)\n","            \n","            #print('Avg test acc is:, %d %%' % (100 * correct / test_len))\n","            print('Avg test acc is:, %d %%' % (round(np.mean(accs), 4) / test_len))\n","\n","            \n","\n","    except KeyboardInterrupt:\n","        sys.exit()\n","    \n","    filename = 'gated_models/' + str(args.dataset) + '/model.' + str(args.loss_threshold) + '.pt'\n","    torch.save(model, filename)"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"code","id":"g64rxk8VxyEw","executionInfo":{"status":"error","timestamp":1651874609270,"user_tz":-180,"elapsed":266,"user":{"displayName":"Assaf La","userId":"07226046456222457646"}},"colab":{"base_uri":"https://localhost:8080/","height":245},"outputId":"9fbee158-785b-486a-fb3c-95d09086212f"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-380b1cbd3625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title load_generated_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCustomTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \"\"\"TensorDataset with support of transforms.\n\u001b[1;32m      5\u001b[0m     \"\"\"\n","\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"]}],"source":["#@title load_generated_data\n","\n","class CustomTensorDataset(Dataset):\n","    \"\"\"TensorDataset with support of transforms.\n","    \"\"\"\n","    def __init__(self, tensors, transform=None):\n","        #ipdb.set_trace(context=6)\n","        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n","        self.tensors = tensors\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        x = self.tensors[0][index]\n","\n","        if self.transform:\n","            x = self.transform(x)\n","\n","        y = self.tensors[1][index]\n","        z = self.tensors[2][index]\n","        to = self.tensors[3][index]\n","        \n","        return x, y, z, to\n","\n","    def __len__(self):\n","        return self.tensors[0].size(0)\n","        \n","def load_generated_data(args, shuffle):\n","    target_file_name = 'targets_main_test'\n","    \n","    ground_truth_noisy_dir = args.ground_truths_dir + '/' + str(args.loss_threshold)# + '/' + 'snr_' + str(noise_levels[0])\n","    \n","    data_file_test = ground_truth_noisy_dir + '/datas_test.' + str(args.loss_threshold) + '.pt'\n","    data_test = torch.load(data_file_test)\n","    print('loaded data test at: ', data_file_test)\n","    \n","    target_file_test = ground_truth_noisy_dir + '/' + target_file_name + '.' + str(args.loss_threshold) + '.pt'\n","    target_test = torch.load(target_file_test)\n","    print('loaded data test at: ', target_file_test)\n","    \n","    gt_file_test = ground_truth_noisy_dir + '/exit_idx_predictions_test.' + str(args.loss_threshold) + '.pt'\n","    exits_ground_truth_test = torch.load(gt_file_test)\n","    print('loaded exits ground truths test at: ', gt_file_test)\n","    \n","    levels_file_test = ground_truth_noisy_dir + '/levels_test.' + str(args.loss_threshold) + '.pt'\n","    levels_test = torch.load(levels_file_test)\n","    #ipdb.set_trace(context=6)\n","    print('levels test at: ', levels_file_test)\n","\n","    validation_set = CustomTensorDataset(tensors=(torch.from_numpy(data_test), torch.from_numpy(exits_ground_truth_test), \n","                                                  torch.from_numpy(levels_test), torch.from_numpy(target_test)))\n","    loader = torch.utils.data.DataLoader(validation_set, batch_size=args.test_batch, shuffle=shuffle)\n","\n","    return validation_set, loader"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"KVOV4gBpm8sD"},"outputs":[],"source":["#@title generate_relative_loss\n","import xgboost as xgb\n","import joblib as jobl\n","\n","def generate_relative_loss(args, main_model, noise_levels):\n","    losses = []\n","    acum_costs = []\n","    noise_exits = []\n","    #vals_len_true = np.zeros(args.num_ee+1)\n","    for i in range(args.num_noise_levels):\n","        noise_exits.append([])\n","\n","    validation_set, loader = load_generated_data(args, shuffle=True)\n","    \n","    log_file = open(args.relative_losses_dir+'/relative_losses_log.txt', 'a', newline='')\n","    log_file.write(\"Generate relative losses:\\n\")\n","    log_file.write(\"loss threshold: \" + str(args.loss_threshold) + \", SNR=\" + str(args.noise_snr) + \"\\n\")\n","    \n","    type_loader = 'test'\n","        \n","    #ipdb.set_trace(context=6)\n","    #args.device = 'cpu'\n","    #main_model.to(args.device)\n","\n","    filename = args.gated_models_dir + '/xg_model.' + str(args.loss_threshold) + '.depth' + str(args.max_depth) + '.json'\n","    #gated_model = jobl.load(filename)\n","    gated_model = xgb.XGBRegressor()\n","    gated_model.load_model(filename)\n","    print('loaded gated model: ', filename)\n","    main_model.eval()\n","    main_model.set_full_flow(True)\n","    total_flops, _ = main_model.complexity[-1]\n","    #ipdb.set_trace(context=6)\n","    target_match_count = 0\n","    \n","    #it = 0\n","    with torch.no_grad():\n","        for data, target, noise, labels in loader:\n","            data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n","            labels = labels.to(args.device, dtype=torch.int64)  \n","            noise = noise.to(args.device, dtype=torch.int64)  \n","            preds, _, _, _ = main_model(data)\n","            \n","            data = torch.flatten(data, start_dim=1)\n","            data_xgb = data.cpu().detach().numpy()\n","            target_xgb = target.cpu().detach().numpy()\n","            xgb_data = xgb.DMatrix(data_xgb, label=target_xgb)\n","            gated_pred = gated_model.predict(data_xgb).astype(int)\n","            #ipdb.set_trace(context=6)\n","            \n","            new_preds = torch.empty((args.test_batch, args.num_ee), device = args.device)\n","            gated_cost = np.zeros(args.test_batch)\n","            for i in range(args.test_batch):\n","                #ipdb.set_trace(context=6)\n","                new_preds[i] = preds[gated_pred[i]][i]\n","                gated_cost[i] = main_model.complexity[gated_pred[i]][0]/total_flops\n","                for j in range(args.num_noise_levels):\n","                    if noise_levels[j] == noise[i]:\n","                        #ipdb.set_trace(context=6)\n","                        noise_exits[j].append(gated_pred[i])\n","            \n","            pred_targets = torch.max(new_preds, 1).indices\n","            _, target_main = torch.max(preds[args.num_ee], 1)\n","                \n","            #cum_loss = F.nll_loss(preds[gated_pred].log(), target)\n","            cum_loss = F.nll_loss(new_preds.log(), target_main)\n","            losses.append(float(cum_loss))\n","            acum_costs.append(gated_cost)\n","            target_match_count = target_match_count + torch.sum(pred_targets==labels)\n","            #ipdb.set_trace(context=6)\n","            \n","            #print('batch idx = ', it)\n","            #it = it + 1\n","        \n","        mean_rel_loss = round(np.mean(losses), 4)\n","        print('Mean relative loss: {:.4f}'.format(mean_rel_loss))\n","        log_file.write(\"Mean relative loss: \" + str(mean_rel_loss) + \"\\n\")\n","        mean_rel_cost = round(np.mean(acum_costs), 4)\n","        print('Mean relative cost: {:.4f}'.format(mean_rel_cost))\n","        log_file.write(\"Mean relative cost: \" + str(mean_rel_cost) + \"\\n\")\n","        accu = target_match_count.cpu().detach()/len(validation_set)\n","        log_file.write(\"Accuracy: \" + str(accu) + \"\\n\")\n","        print('Accuracy: {:.4f}'.format(float(accu)))\n","        torch.save(noise_exits, args.relative_losses_dir + '/noise_exits_' + type_loader + '.' + str(args.loss_threshold) + '.pt')\n","        \n","        #plot_noise_exits(args, noise_exits, args.relative_losses_dir)\n","        return mean_rel_loss, mean_rel_cost, accu\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"JbCDLL1R7Xzy"},"outputs":[],"source":["#@title calc_relative_time\n","import xgboost as xgb\n","import joblib as jobl\n","import json\n","\n","def calc_relative_time(args, main_model, noise_levels):\n","    losses = []\n","    acum_costs = []\n","    noise_exits = []\n","    #vals_len_true = np.zeros(args.num_ee+1)\n","    for i in range(args.num_noise_levels):\n","        noise_exits.append([])\n","    \n","    log_file = open(args.relative_losses_dir+'/calc_relative_time.txt', 'a', newline='')\n","    log_file.write(\"Calc relative time:\\n\")\n","    log_file.write(\"loss threshold: \" + str(args.loss_threshold) + \", SNR=\" + str(args.noise_snr) + \"\\n\")\n","    type_loader = 'test'\n","        \n","    validation_set, loader = load_generated_data(args, shuffle=False)\n","    \n","    #ipdb.set_trace(context=6)\n","    #args.device = 'cpu'\n","    #main_model.to(args.device)\n","\n","    #filename = args.gated_models_dir + '/xg_model.' + str(args.loss_threshold) + '.bin'\n","    filename = args.gated_models_dir + '/xg_model.' + str(args.loss_threshold) + '.depth' + str(args.max_depth) + '.json'\n","    #gated_model = jobl.load(filename)\n","    gated_model = xgb.XGBRegressor()\n","    gated_model.load_model(filename)\n","\n","    #gated_model = xgb.Booster()\n","    #gated_model.load_model(filename)\n","    print('loaded gated model: ', filename)\n","    #gated_model.set_param({\"predictor\": \"gpu_predictor\"})\n","    #gated_model.set_param({\"tree_method\": \"hist\"})\n","    #gated_model.set_param({\"single_precision_histogram\": True})\n","    main_model.eval()\n","    main_model.set_full_flow(True)\n","    total_flops, _ = main_model.complexity[-1]\n","    #ipdb.set_trace(context=6)\n","    target_match_count = 0\n","\n","    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n","    \n","    #it = 0\n","    with torch.no_grad():\n","        for data, target, noise, labels in loader:\n","            data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n","            labels = labels.to(args.device, dtype=torch.int64)  \n","            noise = noise.to(args.device, dtype=torch.int64)\n","            #dummy_input = torch.randn(1, data.shape[1],dtype=torch.float).to(args.device)\n","\n","            \n","            starter.record()  \n","            preds, _, costs, _ = main_model(data)\n","            ender.record()\n","            torch.cuda.synchronize()\n","            curr_time = starter.elapsed_time(ender)\n","            print('avg time main:', curr_time/args.test_batch)\n","            \n","            data = torch.flatten(data, start_dim=1)\n","            data_xgb = data.cpu().detach().numpy()\n","            target_xgb = target.cpu().detach().numpy()\n","            xgb_data = xgb.DMatrix(data_xgb, label=target_xgb)\n","\n","            #ipdb.set_trace(context=6)  \n","            #GPU-WARM-UP\n","            #for _ in range(10):\n","            #    _,_,_,_ = main_model(dummy_input)    \n","            \n","            starter.record()\n","            gated_pred = gated_model.predict(data_xgb)\n","            ender.record()\n","            torch.cuda.synchronize()\n","            curr_time = starter.elapsed_time(ender)\n","            print('avg time xg:', curr_time/args.test_batch)\n","            \n","            #ipdb.set_trace(context=6)\n","            \"\"\"\n","            total_cost = 0\n","            \n","            gated_pred = gated_pred.astype(int)\n","            new_preds = torch.empty((args.test_batch, args.num_ee), device = args.device)\n","            #gated_cost = np.zeros(args.test_batch)\n","            for i in range(args.test_batch):\n","                ipdb.set_trace(context=6)\n","                total_cost = total_cost + costs[gated_pred[i]][i]\n","                new_preds[i] = preds[gated_pred[i]][i]\n","                #gated_cost[i] = main_model.complexity[gated_pred[i]][0]/total_flops\n","                #for j in range(args.num_noise_levels):\n","                #    if noise_levels[j] == noise[i]:\n","                #        #ipdb.set_trace(context=6)\n","                #        noise_exits[j].append(gated_pred[i])\n","            \n","            pred_targets = torch.max(new_preds, 1).indices\n","            _, target_main = torch.max(preds[args.num_ee], 1)\n","            \n","            \n","            avg_cost = total_cost/args.test_batch \n","            print(\"avg cost: \", avg_cost)  \n","            target_match_count = target_match_count + torch.sum(pred_targets==pred_targets)\n","            accu = target_match_count.cpu().detach()/len(validation_set)\n","            log_file.write(\"Accuracy: \" + str(accu) + \"\\n\")\n","            print('Accuracy: {:.4f}'.format(float(accu)))\n","            \"\"\"\n","            \"\"\"\n","            #cum_loss = F.nll_loss(preds[gated_pred].log(), target)\n","            cum_loss = F.nll_loss(new_preds.log(), target_main)\n","            losses.append(float(cum_loss))\n","            acum_costs.append(gated_cost)\n","            target_match_count = target_match_count + torch.sum(pred_targets==labels)\n","            #ipdb.set_trace(context=6)\n","            \"\"\"\n","            #print('batch idx = ', it)\n","            #it = it + 1\n","        \"\"\"\n","        mean_rel_loss = round(np.mean(losses), 4)\n","        print('Mean relative loss: {:.4f}'.format(mean_rel_loss))\n","        log_file.write(\"Mean relative loss: \" + str(mean_rel_loss) + \"\\n\")\n","        mean_rel_cost = round(np.mean(acum_costs), 4)\n","        print('Mean relative cost: {:.4f}'.format(mean_rel_cost))\n","        log_file.write(\"Mean relative cost: \" + str(mean_rel_cost) + \"\\n\")\n","        \"\"\"\n","        return\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_RxrxOmgGRto"},"outputs":[],"source":["#@title Validate\n","def validate(args, model, val_loader):\n","    \"\"\"validate the model.\n","\n","    Arguments are\n","    * args:         command line arguments entered by user.\n","    * model:        convolutional neural network model.\n","    * val_loader:   validation data loader..\n","\n","    This validates the model and prints the results of each epochs.\n","    Finally, it returns average accuracy, loss and comptational cost.\n","    \"\"\"\n","    batch = {'time':[], 'cost':[], 'flop':[], 'acc':[], 'val_loss':[]}\n","    exit_points = [0]*(args.num_ee+1)\n","    # switch to evaluate mode\n","    model.eval()\n","    if args.loss_threshold != 0:\n","        model.set_full_flow(True)\n","    with torch.no_grad():\n","        #for data, target in val_loader:\n","        for data, target, _ in val_loader:\n","            #if args.add_noise != 0:\n","            #    data = add_noise(args, data)\n","        \n","            data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n","            # compute output\n","            start = time.process_time()\n","\n","            # results of EENet based models\n","            if isinstance(model, (EENet, CustomEENet)):                \n","                pred, conf, cost, idx = model(data)\n","                elapsed_time = time.process_time() - start\n","                #loss = F.cross_entropy(pred, target)\n","                if args.loss_threshold != 0:\n","                    idx = args.num_ee\n","                    for i in range(args.num_ee):\n","                        pred_loss = F.nll_loss(pred[i].log(), target)\n","                        #print(pred_loss)\n","                        #return\n","                        if pred_loss < args.loss_threshold:\n","                            idx = i\n","                            #print('target:' + str(target) + ', ee:' + str(i))\n","                            break\n","                    pred = pred[idx]\n","                    cost = cost[idx]\n","                loss = F.nll_loss(pred.log(), target) + args.lambda_coef * cost\n","                flop, cost_ = model.complexity[-1][0], 1.0\n","                exit_points[idx] += 1\n","\n","            # results of other models\n","            else:\n","                pred = model(data)\n","                elapsed_time = time.process_time() - start\n","                loss = F.cross_entropy(pred, target)\n","                flop, cost = model.complexity[-1][0], 1.0\n","                exit_points = None\n","\n","            # get the index of the max log-probability\n","            pred = pred.max(1, keepdim=True)[1]\n","            acc = pred.eq(target.view_as(pred)).sum().item()\n","            batch['acc'].append(acc*100.)\n","            batch['time'].append(elapsed_time)\n","            batch['cost'].append(cost*100.)\n","            batch['flop'].append(flop)\n","            batch['val_loss'].append(float(loss))\n","            \n","    utils.print_validation(args, batch, exit_points)\n","    model.set_full_flow(False)\n","\n","    result = {}\n","    for key, value in batch.items():\n","        result[key] = round(np.mean(value), 4)\n","        result[key+'_sem'] = round(stats.sem(value), 2)\n","    \n","    result['exit_points'] = exit_points\n","    return result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jItHKaEaILYJ"},"outputs":[],"source":["#@title generate_ee_exit_prediction_ground_truths\n","def generate_ee_exit_prediction_ground_truths(args, model, train_loader, test_loader, noise_levels):\n","    \n","    model.eval()\n","    model.set_full_flow(True)\n","    print('shuffle train: ', args.shuffle_train)\n","    \n","    #for m in range(args.num_noise_levels):\n","    type_loader = 'train'\n","    loader = train_loader\n","    for k in range(2):\n","        first = True\n","        if k == 1:\n","            loader = test_loader\n","            type_loader = 'test'\n","        exit_idx_predictions = []\n","        noise_idxs = []\n","        datas = []\n","        #targets_org = []\n","        targets_main = []\n","        noise_lvls = []\n","        loss_main = []\n","        val_confs_true = []\n","        vals_len_true = np.zeros(args.num_ee+1)\n","        for i in range(args.num_ee+1):\n","            val_confs_true.append([])\n","        \n","        print('processing loader type: ', type_loader)\n","        print('batch size: {:d}'.format(args.batch_size))\n","        #print('noise level index : {:d}'.format(m))\n","\n","        with torch.no_grad():\n","            for data, _, levels in loader:\n","                #if args.add_noise != 0:\n","                #    args.noise_snr = noise_levels[m]\n","                #    data = add_noise(args, data)\n","                #ipdb.set_trace(context=6)\n","                #data, target_org = data.to(args.device), target_org.to(args.device, dtype=torch.int64)\n","                data = data.to(args.device)\n","                preds, _, _, _ = model(data)\n","                \n","                if args.use_main_targets:\n","                    _, target_main = torch.max(preds[args.num_ee], 1)\n","                \n","                #ipdb.set_trace(context=6)\n","                best_exit = np.full(target_main.shape[0], args.num_ee, dtype=int) #args.num_ee\n","                for j in range(target_main.shape[0]):\n","                    for i in range(args.num_ee):\n","                        pred = preds[i][j][None, :]\n","                        pred_loss = F.nll_loss(pred.log(), target_main[j].unsqueeze(0))\n","                    \n","                        if pred_loss < args.loss_threshold:\n","                            best_exit[j] = i\n","                            break\n","\n","                    val_confs_true[best_exit[j]].append(pred_loss)\n","\n","                #ipdb.set_trace(context=6)\n","                exit_idx_predictions = np.append(exit_idx_predictions, best_exit)\n","                \n","                if first:\n","                    first = False\n","                    datas = data.cpu().detach().numpy()\n","                else:\n","                    datas = np.append(datas, data.cpu().detach().numpy(), axis=0)\n","                \n","                noise_lvls = np.append(noise_lvls, levels.cpu().detach().numpy())\n","                #targets_org = np.append(targets_org, target_org.cpu().detach().numpy())\n","                targets_main = np.append(targets_main, target_main.cpu().detach().numpy())\n","                #noise_idxs = np.append(noise_idxs, noise_level_idx.cpu().detach().numpy())\n","                loss_main.append(float(F.nll_loss(preds[args.num_ee].log(), target_main)))\n","                \n","        print('exit_idx_predictions length {:8d}'.format(len(exit_idx_predictions)))\n","        #print('noise_idxs length {:8d}'.format(len(noise_idxs)))\n","        print('loss_main mean: {:.2f}'.format(round(np.mean(loss_main), 4)))\n","        \n","        ground_truth_noisy_dir = args.ground_truths_dir + '/' + str(args.loss_threshold)# + '/' + 'snr_' + str(noise_levels[m])\n","        if not os.path.exists(ground_truth_noisy_dir):\n","            os.makedirs(ground_truth_noisy_dir)\n","\n","        torch.save(exit_idx_predictions, ground_truth_noisy_dir\n","                + '/exit_idx_predictions_' + type_loader + '.' + str(args.loss_threshold) + '.pt')\n","        #torch.save(noise_idxs, ground_truth_noisy_dir\n","        #        + '/noise_idxs_' + type_loader + '.' + str(args.loss_threshold) + '.pt')\n","        \n","        if type_loader == 'test':\n","            torch.save(datas, ground_truth_noisy_dir + '/datas_' + type_loader + '.' + str(args.loss_threshold) + '.pt')\n","        torch.save(targets_main, ground_truth_noisy_dir + '/targets_main_' + type_loader + '.' + str(args.loss_threshold) + '.pt')\n","        #torch.save(targets_org, ground_truth_noisy_dir + '/targets_org_' + type_loader + '.' + str(args.loss_threshold) + '.pt')\n","        torch.save(noise_lvls, ground_truth_noisy_dir + '/levels_' + type_loader + '.' + str(args.loss_threshold) + '.pt')\n","        for i in range(args.num_ee+1):\n","            vals_len_true[i] = len(val_confs_true[i])\n","        plot_ee_histogram(args, vals_len_true, type_loader, ground_truth_noisy_dir)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"UTyGwEMtGWT4"},"outputs":[],"source":["#@title run()\n","def run(model, optimizer, args, train_loader, test_loader, exit_tags):\n","    # best = {}\n","    # best_epoch = 0\n","    \n","    print('Running for {:5d} epochs'.format(args.epochs))\n","    try:\n","        if not args.no_tensorboard:\n","            writer = SummaryWriter('../runs/eenet_experiment_1')\n","        torch.cuda.empty_cache()\n","        for epoch in range(args.start_epoch, args.epochs + 1):\n","            print('{:3d}:'.format(epoch), end='')\n","\n","            # two-stage training uses the loss version-1 after training for 25 epochs\n","            if args.two_stage and epoch > 25:\n","                args.loss_func = \"v1\"\n","            \n","            # use adaptive learning rate\n","            if args.adaptive_lr:\n","                utils.adaptive_learning_rate(args, optimizer, epoch)\n","\n","            result = {'epoch':epoch}\n","            #args.noise_snr = noise_levels[epoch%args.num_noise_levels]\n","            #print('noise SNR = ', args.noise_snr)\n","            \n","            result.update(train(args, model, train_loader, optimizer, exit_tags))\n","            if not args.no_tensorboard:\n","                writer.add_scalar(\"Train/loss\", result['train_loss'], epoch)\n","\n","            # validate and keep history at each log interval\n","            if epoch % args.log_interval == 0:\n","                result.update(validate(args, model, test_loader))\n","                utils.save_history(args, result)\n","                if not args.no_tensorboard:\n","                    writer.add_scalar(\"Val/loss\", result['val_loss'], epoch)\n","                    writer.add_scalars('Val/acc_cost', {\n","                        'acc': result['acc'],\n","                        'cost': result['cost']\n","                    }, epoch)\n","            if not args.no_tensorboard:    \n","                writer.flush()\n","            # save model parameters\n","            if not args.no_save_model:\n","                utils.save_model(args, model, epoch)\n","    except KeyboardInterrupt:\n","        utils.close_history(args)\n","        utils.plot_history(args)\n","        if not args.no_tensorboard:\n","            writer.close()\n","        sys.exit()\n","    # print the best validation result\n","    best_epoch = utils.close_history(args)\n","\n","    # save the model giving the best validation results as a final model\n","    if not args.no_save_model:\n","        utils.save_model(args, model, best_epoch, True)\n","    utils.plot_history(args)\n","    if not args.no_tensorboard:\n","        writer.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"6cWNwDHcGa8Y"},"outputs":[],"source":["#@title enable_branches_training_only\n","def enable_branches_training_only(model, args):\n","    if args.model == 'eenet8':\n","        model.set_ee_disable(False)\n","        model.initblock.requires_grad_(False)\n","        model.basicblock1.requires_grad_(False)\n","        model.basicblock2.requires_grad_(False)\n","        model.basicblock3.requires_grad_(False)\n","        model.finalblock.requires_grad_(False)\n","        model.classifier.requires_grad_(False)\n","        model.conv2d_6.requires_grad_(False)\n","        model.conv2d_9.requires_grad_(False)\n","    else:\n","        model.set_ee_disable(False)\n","        for idx, exitblock in enumerate(model.exits):\n","            model.stages[idx].requires_grad_(False)\n","            for param in model.exits.parameters():\n","                param.requires_grad = True\n","            #model.exits.requires_grad(True)\n","\n","        model.stages[-1].requires_grad_(False)\n","        model.classifier.requires_grad_(False)\n","        model.confidence.requires_grad_(False)\n","        #params = model_post.state_dict()\n","        #print(params)\n","        \"\"\"\n","        for name, p in model.named_parameters():\n","            if p.requires_grad:\n","                print(name, p.requires_grad)\n","        \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"gHiVW1NCA5JT"},"outputs":[],"source":["#@title plot_relative_loss\n","def plot_relative_loss():\n","    rel_losses_no_noise = np.array([0.2629, 0.2704, 0.2939, 0.3123, 0.3423, 0.3687, 0.3952, 0.4234, 0.4565,\n","        0.4912, 0.519])\n","    rel_losses_20db = np.array([0.2847, 0.3419, 0.3947, 0.4448, 0.4891, 0.523,  0.5664, \n","        0.615,  0.6568, 0.6988, 0.7474])\n","    rel_losses_10db = np.array([0.2886, 0.3495, 0.3812, 0.4199, 0.4694, 0.5304, 0.5900, 0.6455,\n","        0.7120, 0.7603, 0.8047])\n","    rel_losses_30db = np.array([0.2496, 0.3067, 0.3593, 0.4223, 0.4915, 0.5604, 0.5943, 0.6343, \n","        0.6828, 0.728, 0.7757])\n","    \n","    \n","    #print('rel_losses: ', rel_losses)\n","    main_loss_no_noise = 0.12\n","    main_loss_20db = 0.14\n","    main_loss_30db = 0.14\n","    main_loss_10db = 0.14\n","\n","    x = np.arange(0.2, 1.3, 0.1)\n","    \n","    y_no_noise = rel_losses_no_noise/main_loss_no_noise\n","    x_no_noise = x/main_loss_no_noise\n","    y_20db = rel_losses_20db/main_loss_20db\n","    x_20db = x/main_loss_20db\n","    y_10db = rel_losses_10db/main_loss_10db\n","    x_10db = x/main_loss_10db\n","    y_30db = rel_losses_30db/main_loss_30db\n","    x_30db = x/main_loss_30db\n","    \n","    %matplotlib inline\n","    plt.title(\"Test loss vs loss threshold increase\")\n","    plt.xlabel(\"loss threshold\")\n","    plt.ylabel(\"test loss\")\n","    plt.xlim(0, max(x_no_noise)+1)\n","    plt.ylim(0, max(y_no_noise)+3)\n","    xpoints = ypoints = plt.xlim()\n","    plt.gca().set_aspect('equal', adjustable='box')\n","    plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=3, scalex=False, scaley=False)\n","    plt.plot(x_no_noise, y_no_noise, color =\"red\", marker='o')\n","    plt.plot(x_20db, y_20db, color =\"blue\", marker='x')\n","    plt.plot(x_10db, y_10db, color =\"green\", marker='x')\n","    plt.plot(x_30db, y_30db, color =\"cyan\", marker='x')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9SaYNXpxXzhT"},"outputs":[],"source":["#@title load data\n","def load_data(args):\n","    if args.add_noise == 0:\n","        print('load plain dataset')\n","        return utils.load_dataset(args)\n","    else:\n","        print('load noisey dataset')\n","        return utils.load_noisy_dataset(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1641128360325,"user":{"displayName":"Assaf La","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggg7DEAOambuBZkmwZ_9nyzKIhOJ9xl0skw55KG490=s64","userId":"07226046456222457646"},"user_tz":-120},"id":"TiAPblzAGeLz","outputId":"1df4eecf-cc70-4746-b132-760a0f0d3cd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["#@title main\n","\n","\"\"\"Main function of the program.\n","\n","The function loads the dataset and calls training and validation functions.\n","\"\"\"\n","%load_ext autoreload\n","%autoreload 2\n","\n","import importlib\n","importlib.reload(config)\n","importlib.reload(loss_functions)\n","importlib.reload(utils)\n","#from eenet import EENet\n","#from gatednet import GatedNet, GatedNetS\n","from enum import Enum\n","\n","class Mode(Enum):\n","    train_main = 0\n","    train_ee = 1\n","    generate_exits_gt = 2 \n","    train_gating = 3\n","    generate_relative_loss = 4 \n","    plot_relative_loss = 5\n","    calc_relative_time = 6\n","\n","def main(mode: Mode):\n","    print(mode.value)\n","    args = config.args_global\n","    args += config.argu[mode.value]\n","    \n","    print(args)\n","    %pwd\n","    model, optimizer, args = initializer(args)\n","    noise_levels = np.arange(args.min_noise_snr, args.max_noise_snr,\n","            (args.max_noise_snr - args.min_noise_snr)/args.num_noise_levels) \n","    print(noise_levels)\n","\n","    if mode == Mode.train_main:\n","        train_loader, test_loader, exit_tags, trainset, testset = load_data(args)\n","        model.set_ee_disable(True)\n","        print('Disabled EE branches')\n","        run(model, optimizer, args, train_loader, test_loader, exit_tags)\n","    elif mode == Mode.train_ee:\n","        train_loader, test_loader, exit_tags, trainset, testset = load_data(args)\n","        enable_branches_training_only(model, args)\n","        print('Enabled EE branches')\n","        run(model, optimizer, args, train_loader, test_loader, exit_tags)\n","    elif mode == Mode.generate_exits_gt:\n","        train_loader, test_loader, exit_tags, trainset, testset = load_data(args)       \n","        for thresh in range(1, 13, 1):\n","            args.loss_threshold = thresh/10\n","            print('Generating ground truth for ee loss, threshold = {:.2f}'.format(args.loss_threshold))\n","            generate_ee_exit_prediction_ground_truths(args, model, train_loader, test_loader, noise_levels)\n","    #elif config.mode == 'analyze_ee_data_flow':\n","    #    exit_flow_analysis(args, model, test_loader)\n","    elif mode == Mode.train_gating:\n","        print('Training Gated model')\n","        accus = []\n","        for thresh in range(7, 8, 1):\n","            args.loss_threshold = thresh/10\n","            val_accu = train_gated_xgboost(args, noise_levels)\n","            accus.append(val_accu)\n","        print('val_accuracies: ', np.array(accus))\n","        #train_gating_model(args, model, train_loader, test_loader, trainset)\n","    elif mode == Mode.generate_relative_loss:\n","        print('Generate relative loss')\n","        rel_losses = []\n","        costs = []\n","        accus = []\n","        main_loss = 0.14\n","        for thresh in range(7, 8, 1):\n","            args.loss_threshold = thresh/10\n","            rel_loss, cost, accu = generate_relative_loss(args, model, noise_levels)\n","            rel_losses.append(rel_loss)\n","            costs.append(cost)\n","            accus.append(accu)\n","        #main_loss = 0.14\n","        print('rel_losses: ', np.array(rel_losses))\n","        print('costs: ', np.array(costs))\n","        print('accus: ', np.array(accus))\n","        #plot_relative_loss(np.array(rel_losses), main_loss)\n","    elif mode == Mode.plot_relative_loss:\n","        print('Plot relative loss')\n","        dataset_type = 'test'\n","        plot_noise_exits_scatter(args, dataset_type)\n","    elif mode == Mode.calc_relative_time:\n","        print('Calc relative time')\n","        for thresh in range(7, 8, 1):\n","            args.loss_threshold = thresh/10\n","            calc_relative_time(args, model, noise_levels)\n","\n","    print('Finished')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31437,"status":"ok","timestamp":1641128391755,"user":{"displayName":"Assaf La","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggg7DEAOambuBZkmwZ_9nyzKIhOJ9xl0skw55KG490=s64","userId":"07226046456222457646"},"user_tz":-120},"id":"v9gjbi-OCQ2J","outputId":"024cc0a8-fded-404f-91ce-6db109292bb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["6\n","['--dataset', 'cifar10', '--model', 'eenet110', '--num-ee', '10', '--filters', '4', '--exit-type', 'conv2', '--distribution', 'fine', '--add-noise', '--min-noise-snr', '-5', '--max-noise-snr', '30', '--num-noise-levels', '5', '--test-batch', '10000', '--load-model', 'models/cifar10/eenet110/UT/full_noisy/after_ee_training/model.pt', '--max-depth', '10']\n","use cuda:  True  device:  cuda\n","empty model loaded at:  models/cifar10/eenet110/UT/full_noisy/after_ee_training/model.pt\n","ee-block-0: flops=19.78 MMac, params=19.34 k, cost-rate=0.08\n","ee-block-1: flops=34.23 MMac, params=33.35 k, cost-rate=0.13\n","ee-block-2: flops=43.86 MMac, params=42.7 k, cost-rate=0.17\n","ee-block-3: flops=53.49 MMac, params=52.04 k, cost-rate=0.21\n","ee-block-4: flops=63.13 MMac, params=61.39 k, cost-rate=0.25\n","ee-block-5: flops=72.76 MMac, params=70.73 k, cost-rate=0.28\n","ee-block-6: flops=82.4 MMac, params=80.08 k, cost-rate=0.32\n","ee-block-7: flops=92.03 MMac, params=89.42 k, cost-rate=0.36\n","ee-block-8: flops=100.48 MMac, params=136.57 k, cost-rate=0.39\n","ee-block-9: flops=110.01 MMac, params=173.69 k, cost-rate=0.43\n","exit-block: flops=256.32 MMac, params=1.73 M, cost-rate=1.00\n","[-5.  2.  9. 16. 23.]\n","Calc relative time\n","loaded data test at:  gated_ground_truths/cifar10/levels_5/0.7/datas_test.0.7.pt\n","loaded data test at:  gated_ground_truths/cifar10/levels_5/0.7/targets_main_test.0.7.pt\n","loaded exits ground truths test at:  gated_ground_truths/cifar10/levels_5/0.7/exit_idx_predictions_test.0.7.pt\n","levels test at:  gated_ground_truths/cifar10/levels_5/0.7/levels_test.0.7.pt\n","loaded gated model:  gated_models/cifar10/snr_0/xg_model.0.7.depth10.json\n","avg time main: 0.2167572998046875\n","avg time xg: 0.13804256591796876\n","avg time main: 0.19455714111328126\n","avg time xg: 0.11938775634765625\n","avg time main: 0.19455770263671876\n","avg time xg: 0.12093385009765625\n","avg time main: 0.1945577392578125\n","avg time xg: 0.121714892578125\n","avg time main: 0.19455289306640625\n","avg time xg: 0.120680078125\n","Finished\n"]}],"source":["mode = Mode.calc_relative_time\n","main(mode)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"colab_main.ipynb","provenance":[]},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.7.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"metadata":{"interpreter":{"hash":"4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"}}},"nbformat":4,"nbformat_minor":0}